# MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation [ECCV2020]
by Kaisiyuan Wang, [Qianyi Wu](https://wuqianyi.top/), Linsen Song, [Zhuoqian Yang](https://yzhq97.github.io/), [Wayne Wu](https://wywu.github.io/), [Chen Qian](https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en), [Ran He](https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en), [Yu Qiao](https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en), [Chen Change Loy](http://personal.ie.cuhk.edu.hk/~ccloy/).
## Introduction
This repository is for our ECCV2020 paper [MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation] (https://wywu.github.io/projects/MEAD/support/MEAD.pdf). The code is modified following [MUNIT](https://github.com/NVlabs/MUNIT).
## Installation 
This repository is based on Pytorch, so please follow the official instructions in [here](https://pytorch.org/). The code is tested under pytorch1.0 and Python 3.6 on Ubuntu 16.04.

## Usage
### Training
As Mead requires different modules to achieve different functions, thus we seperate the training for Mead into three stages.
#### Stage 1: Audio-to-Landmarks Module

#### Stage 2: Neutral-to-Emotion Transformer

#### Stage 3: Refinement Network

### Testing



## Citation
If you find this code useful for your research, please cite our paper:
